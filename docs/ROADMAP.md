# Roadmap & Progress

## üèÅ Major Milestones

- ‚úÖ Petals/Hivemind compiles and installs on Windows (distributed inference not functional)
- ‚úÖ BareMetalRT CLI validated for local inference
- ‚úÖ FastAPI REST API operational for local inference
- ‚úÖ Model loading and quantized inference validated (Llama 2 7B, Mistral 7B, GPT-2)
- ‚è∏Ô∏è Distributed inference (Petals/Hivemind) deferred pending DHT/network compatibility

## 1. Local Inference

- 1.1 ‚úÖ Validate Hugging Face Transformers (PyTorch) backend for local inference (Llama 2 7B, Mistral 7B, GPT-2)
- 1.2 ‚úÖ Validate BareMetalRT CLI for local inference (TensorRT-LLM backend)
- 1.3 ‚¨ú Add quantized/fp16 model support for both backends
- 1.4 ‚¨ú Document hardware requirements and performance for each backend
- 1.5 ‚úÖ Integrate local inference with FastAPI REST API (OpenAI-compatible, robustly tested)

## 2. Node Architecture & Logic

- 2.1 ‚úÖ Modularize node/server architecture for backend selection (Petals, BareMetalRT, future backends)
- 2.2.1 ‚úÖ Refactor node logic for single-node/local inference
- 2.2.2 ‚è∏Ô∏è Refactor node logic for distributed inference (deferred)
- 2.3 ‚¨ú Integrate peer discovery and job routing layer (Hivemind, p2pd, ZeroMQ research)
- 2.4 ‚¨ú Document and test node startup, backend selection, and routing logic
- 2.5 ‚¨ú Ensure all patches/stubs for Windows compatibility are tracked and reproducible

## 3. Distributed Inference (Deferred)

- 3.1 ‚è∏Ô∏è Test multi-node setup (multiple Windows machines or processes) *(Deferred: Windows DHT/distributed support is currently on hold due to compatibility blockers)*
- 3.2 ‚è∏Ô∏è Validate DHT peer discovery and layer assignment
- 3.3 ‚è∏Ô∏è Confirm end-to-end distributed inference (prompt ‚Üí output)

## 4. Web Integration

- 4.1 ‚úÖ Add a REST inference API server (FastAPI) for single-node/local inference
- 4.2 ‚úÖ Define and document OpenAI-compatible endpoint(s) (`/v1/completions`) for prompt submission and completion retrieval (TensorRT-LLM style)
- 4.3 ‚¨ú Integrate Chatbot UI frontend with the OpenAI-compatible inference API (`openai_api`) (MVP: single-user, no RAG, no multi-user)
- 4.4 ‚¨ú Test end-to-end integration (web UI ‚Üí API ‚Üí model ‚Üí UI)
- 4.5 ‚¨ú Implement authentication and CORS for API security
- 4.6 ‚¨ú Document setup and usage in README/API.md


## 5. Lightweight Optimization

- 5.1 ‚¨ú Remove or stub unnecessary features for MVP (e.g., metrics, advanced monitoring, Docker)
- 5.2 ‚¨ú Optimize dependency list for minimal install
- 5.3 ‚¨ú Document resource usage and performance

## 6. Testing & Validation

- 6.1 ‚¨ú Write/check basic test cases for single-node and multi-node inference
- 6.2 ‚¨ú Validate outputs match expectations for Llama 70B
- 6.3 ‚¨ú Document known issues and workarounds

## 7. Documentation & Packaging

- 7.1 ‚¨ú Update `README.md` with Windows-specific setup and usage
- 7.2 ‚¨ú Document all patches and stubs in this `agent.md`
- 7.3 ‚¨ú Provide troubleshooting and FAQ section
- 7.4 ‚¨ú Keep `windows_patches.md` up to date with every technical decision and patch
